{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Georgian Wine Expert: Model Inference and Comparison\n",
    "\n",
    "This notebook demonstrates the fine-tuned Georgian wine specialist model compared to the base Flan-T5 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch --quiet\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base Flan-T5 model\n",
    "print(\"Loading base Flan-T5 model...\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Load fine-tuned model\n",
    "print(\"Loading fine-tuned Georgian wine model...\")\n",
    "expert_tokenizer = AutoTokenizer.from_pretrained(\"./georgian-wine-model\")\n",
    "expert_model = AutoModelForSeq2SeqLM.from_pretrained(\"./georgian-wine-model\")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_model.to(device)\n",
    "expert_model.to(device)\n",
    "\n",
    "print(f\"\\nâœ… Models loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Query Function (Required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_expert(question: str, model_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes a question about Georgian wine and returns a knowledgeable response.\n",
    "    \n",
    "    Args:\n",
    "        question: A question about Georgian winemaking\n",
    "        model_path: Path to the fine-tuned model directory\n",
    "    \n",
    "    Returns:\n",
    "        Model's response as a string\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Format input\n",
    "    input_text = f\"Answer this question about Georgian wine: {question}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=256,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "print(\"âœ… ask_expert() function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Function for Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(question: str):\n",
    "    \"\"\"\n",
    "    Compare base Flan-T5 response with fine-tuned expert model.\n",
    "    \"\"\"\n",
    "    # Format input\n",
    "    input_text = f\"Answer this question about Georgian wine: {question}\"\n",
    "    \n",
    "    # Base model response\n",
    "    base_inputs = base_tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "    base_inputs = {k: v.to(device) for k, v in base_inputs.items()}\n",
    "    \n",
    "    base_outputs = base_model.generate(\n",
    "        **base_inputs,\n",
    "        max_length=256,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    base_response = base_tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Expert model response\n",
    "    expert_inputs = expert_tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "    expert_inputs = {k: v.to(device) for k, v in expert_inputs.items()}\n",
    "    \n",
    "    expert_outputs = expert_model.generate(\n",
    "        **expert_inputs,\n",
    "        max_length=256,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    expert_response = expert_tokenizer.decode(expert_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Display comparison\n",
    "    print(\"=\"*100)\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(\"=\"*100)\n",
    "    print(\"\\nðŸ”µ BASE FLAN-T5 (No fine-tuning):\")\n",
    "    print(\"-\"*100)\n",
    "    print(base_response)\n",
    "    print(\"\\nðŸŸ¢ FINE-TUNED GEORGIAN WINE EXPERT:\")\n",
    "    print(\"-\"*100)\n",
    "    print(expert_response)\n",
    "    print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "print(\"âœ… compare_models() function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Side-by-Side Comparison: Base vs Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 1\n",
    "compare_models(\"What is a qvevri?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 2\n",
    "compare_models(\"How old is Georgian winemaking tradition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 3\n",
    "compare_models(\"What is Saperavi and where is it grown?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 4\n",
    "compare_models(\"What is the difference between Kakhetian and Imeretian winemaking styles?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 5\n",
    "compare_models(\"Why does Georgian white wine sometimes look orange?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 6\n",
    "compare_models(\"How is a qvevri cleaned and maintained?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 7\n",
    "compare_models(\"When was Georgian winemaking added to UNESCO's heritage list?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 8\n",
    "compare_models(\"Why are qvevri buried underground?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 9\n",
    "compare_models(\"What is Rkatsiteli grape known for?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 10\n",
    "compare_models(\"What foods pair well with Georgian amber wine?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 11\n",
    "compare_models(\"What is the Kakheti wine region known for?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 12\n",
    "compare_models(\"How did Georgian winemaking change during the Soviet era?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the ask_expert() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the ask_expert function\n",
    "question = \"What is skin contact fermentation?\"\n",
    "answer = ask_expert(question, \"./georgian-wine-model\")\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nExpert Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own question\n",
    "custom_question = \"What is Mtsvane?\"  # Change this to any question\n",
    "compare_models(custom_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "âœ… Loading both base and fine-tuned models  \n",
    "âœ… Implementing the `ask_expert()` function as required  \n",
    "âœ… Side-by-side comparison showing the fine-tuned model's superior domain knowledge  \n",
    "âœ… Interactive testing capability  \n",
    "\n",
    "**Key Observations:**\n",
    "- The base Flan-T5 model gives generic or incorrect answers\n",
    "- The fine-tuned model provides accurate, detailed, domain-specific responses\n",
    "- Fine-tuning successfully transferred specialized knowledge to the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
